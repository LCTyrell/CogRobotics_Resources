---
layout: default
title: Datasets
nav_order: 6
---

## 3D Objects Datasets

* [Procedurally Generated Random Objects](https://sites.google.com/site/brainrobotdata/home/models) - Google Brain. In order to enable training of robot grasping and other tasks in simulation, we require a set of sample objects.  We used a set of randomly generated simple rigid objects.  Here we provide 3D models for these objects in OBJ format and URDF representations for use with Bullet. 

## Simulated Robotics Environments

* [Ingredients for Robotics Research](https://openai.com/blog/ingredients-for-robotics-research/#requestsforresearchheredition) - OpenAI. We’re releasing eight simulated robotics environments and a Baselines implementation of Hindsight Experience Replay, all developed for our research over the past year. We’ve used these environments to train models which work on physical robots. 

* [Interactive Gibson Environment](http://svl.stanford.edu/gibson2/) - Stanford. Large Scale Virtualized Interactive Environment for Learning Robot Manipulation and Navigation.

* [AI2-THOR](https://ai2thor.allenai.org/) is built and maintained by the Allen Institute for Artificial Intelligence. With a dedicated team of researchers and software engineers, the AI2-THOR project is uniquely positioned for long-term growth alongside a vibrant open-source development community.

## Vision-Based Robotic Manipulation Datasets

* [Multiview Pouring Dataset](https://sites.google.com/site/brainrobotdata/home/multiview-pouring) - Google Brain. This dataset contains a variety of people pouring liquids into containers, taken from multiple angles, which can be used to learn representations of the abstract task of pouring for robot learning.

* [Grasping Dataset](https://sites.google.com/site/brainrobotdata/home/grasping-dataset) - Google Brain. This dataset contains recordings of 650k robotic grasp attempts. Each grasp attempt is annotated with the success or failure of the grasp. Data was collected using real robots and several hundred different objects.

* [Push Dataset](https://sites.google.com/site/brainrobotdata/home/push-dataset) - Google Brain. This dataset contains recordings of 59k object pushing interactions. Each push includes video and joint angle sequences. Data was collected using real robots and several hundred different objects.

* [RoboNet](https://www.robonet.wiki/) - The standard paradigm in robot learning is to set-up experiments in a single lab environment and train a robot from scratch from data collected in that setting. In contrast, essentially all machine learning fields accumulate and share large datasets across institutions, which enables training of models that generalize much more broadly. We aim to take a step in this direction by collecting a large dataset from 7 robot platforms across multiple institutions, which we call RoboNet. Critically, we find that pre-training on RoboNet enables us to generalize to entirely new robot platforms with less data than training from scratch.

* [Dexterity Network (Dex-Net)](https://berkeleyautomation.github.io/dex-net/) - UC Berkeley. The Dexterity Network (Dex-Net) is a research project including code, datasets, and algorithms for generating datasets of synthetic point clouds, robot parallel-jaw grasps and metrics of grasp robustness based on physics for thousands of 3D object models to train machine learning-based methods to plan robot grasps. The broader goal of the Dex-Net project is to develop highly reliable robot grasping across a wide variety of rigid objects such as tools, household items, packaged goods, and industrial parts. 
